"""Example: LangGraph integration with multi-provider LLM support.

Demonstrates how LangGraph works with any LLM provider through LLMRouter.

Features:
- Multi-provider support (OpenAI, Anthropic, OpenRouter, Ollama)
- Automatic fallback to Ollama on failures
- Provider routing based on task type
- Complete Phase 1-3 integration
"""

import os
from pathlib import Path

# Phase 1-3 imports
from kira.agent import (
    # LangGraph
    LangGraphExecutor,
    # LLM Integration - –∫–ª—é—á–µ–≤–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç!
    create_langgraph_llm_adapter,
    # Tools
    create_tool_executor,
    # Memory & RAG
    create_context_memory,
    create_rag_integration,
    # Persistence
    create_persistence,
    # Safety & Observability
    create_policy_enforcer,
    create_audit_logger,
    create_metrics_collector,
)
from kira.core.host import create_host_api


def example_1_basic_any_provider():
    """Example 1: Basic usage with any LLM provider."""
    print("=" * 60)
    print("Example 1: LangGraph with ANY LLM Provider")
    print("=" * 60)

    # Create LLM adapter - —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –õ–Æ–ë–´–ú –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–º!
    llm = create_langgraph_llm_adapter(
        api_keys={
            "anthropic": os.getenv("ANTHROPIC_API_KEY", ""),
            "openai": os.getenv("OPENAI_API_KEY", ""),
            "openrouter": os.getenv("OPENROUTER_API_KEY", ""),
        },
        # Routing strategy:
        planning_provider="anthropic",  # Claude –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        structuring_provider="openai",  # GPT-4 –¥–ª—è JSON
        default_provider="openrouter",  # OpenRouter –∫–∞–∫ default
        enable_ollama_fallback=True,  # Fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π Ollama
        task_type="planning",  # LangGraph –¥–µ–ª–∞–µ—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
    )

    # Create tools
    vault_path = Path.cwd() / "vault"
    host_api = create_host_api(vault_path)
    tool_executor = create_tool_executor(host_api, vault_path)
    tool_registry = tool_executor.get_tool_registry()

    # Create executor
    executor = LangGraphExecutor(
        llm,  # –ù–∞—à multi-provider –∞–¥–∞–ø—Ç–µ—Ä!
        tool_registry,
        max_steps=10,
        enable_reflection=True,
        enable_verification=True,
    )

    # Execute
    print("\n‚ú® Executing with automatic provider selection...")
    result = executor.execute("Create a task called 'Test Multi-Provider'")

    print(f"\n‚úÖ Success: {result.success}")
    print(f"üìä Status: {result.status}")
    print(f"üîß Tools executed: {len(result.tool_results)}")
    print(f"üí∞ Budget used: {result.budget_used.steps_used} steps")


def example_2_provider_fallback():
    """Example 2: Automatic fallback to Ollama when remote fails."""
    print("\n" + "=" * 60)
    print("Example 2: Automatic Fallback to Ollama")
    print("=" * 60)

    # –ë–µ–∑ API –∫–ª—é—á–µ–π - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ fallback –Ω–∞ Ollama!
    llm = create_langgraph_llm_adapter(
        api_keys={},  # –ü—É—Å—Ç—ã–µ –∫–ª—é—á–∏
        enable_ollama_fallback=True,  # Ollama —Å–ø–∞—Å–∞–µ—Ç!
        task_type="planning",
    )

    print("\nüîÑ No API keys provided - will fallback to Ollama")
    print("üí° Make sure Ollama is running: ollama serve")

    # Rest of setup...
    vault_path = Path.cwd() / "vault"
    host_api = create_host_api(vault_path)
    tool_executor = create_tool_executor(host_api, vault_path)
    tool_registry = tool_executor.get_tool_registry()

    executor = LangGraphExecutor(llm, tool_registry)

    # Execute - –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å Ollama!
    result = executor.execute("List all tasks")

    print(f"\n‚úÖ Fallback worked: {result.success}")


def example_3_full_integration():
    """Example 3: Full integration with all Phase 1-3 components."""
    print("\n" + "=" * 60)
    print("Example 3: Full Integration (Phase 1-3)")
    print("=" * 60)

    vault_path = Path.cwd() / "vault"

    # 1. LLM with multi-provider support
    llm = create_langgraph_llm_adapter(
        api_keys={
            "anthropic": os.getenv("ANTHROPIC_API_KEY", ""),
        },
        enable_ollama_fallback=True,
    )

    # 2. Tools with validation
    host_api = create_host_api(vault_path)
    tool_executor = create_tool_executor(host_api, vault_path)
    tool_registry = tool_executor.get_tool_registry()

    # 3. Memory for multi-turn conversations
    memory = create_context_memory(max_facts=20)

    # 4. RAG for better planning
    rag = create_rag_integration(
        vault_path=vault_path,
        index_path=vault_path / ".kira" / "rag_index.json",
    )

    # 5. Persistence for recovery
    persistence = create_persistence(
        storage_type="sqlite",
        storage_path=vault_path / ".kira" / "agent_states",
    )

    # 6. Policy enforcement
    policy_enforcer = create_policy_enforcer(
        enable_delete=False,  # –ó–∞–ø—Ä–µ—â–∞–µ–º —É–¥–∞–ª–µ–Ω–∏–µ
        require_confirmation=True,
    )

    # 7. Audit logging
    audit_logger = create_audit_logger(
        audit_path=vault_path / "artifacts" / "audit" / "agent",
    )

    # 8. Metrics collection
    metrics = create_metrics_collector()

    # Create executor
    executor = LangGraphExecutor(
        llm,  # Multi-provider LLM
        tool_registry,
        max_steps=15,
        max_tokens=15000,
        enable_reflection=True,
        enable_verification=True,
    )

    print("\nüöÄ Full stack initialized:")
    print(f"   ‚Ä¢ LLM: Multi-provider with fallback")
    print(f"   ‚Ä¢ Tools: {len(tool_registry.list_tools())} validated tools")
    print(f"   ‚Ä¢ Memory: Multi-turn context")
    print(f"   ‚Ä¢ RAG: Documentation-enhanced planning")
    print(f"   ‚Ä¢ Persistence: SQLite state recovery")
    print(f"   ‚Ä¢ Policies: Capability enforcement")
    print(f"   ‚Ä¢ Audit: JSONL event logging")
    print(f"   ‚Ä¢ Metrics: Prometheus-compatible")

    # Execute complex workflow
    trace_id = "full-integration-demo"

    print(f"\nüìù Executing workflow...")
    result = executor.execute(
        "Create a task named 'Integration Test' with tag 'demo'",
        trace_id=trace_id,
    )

    # Save state for recovery
    if result.state:
        persistence.save_state(trace_id, result.state)
        print(f"üíæ State saved for trace: {trace_id}")

    # Log audit event
    audit_logger.log_node_execution(
        trace_id=trace_id,
        node="full_workflow",
        output_data={"success": result.success},
        elapsed_ms=int(result.budget_used.wall_time_used * 1000),
    )

    # Record metrics
    metrics.record_step(success=result.success)
    for tool_result in result.tool_results:
        metrics.record_tool_execution(
            tool_name=tool_result.get("tool", "unknown"),
            latency_seconds=tool_result.get("elapsed_ms", 0) / 1000,
            success=tool_result.get("status") == "ok",
        )

    # Get health
    health = metrics.get_health()

    print(f"\n‚úÖ Execution complete:")
    print(f"   ‚Ä¢ Success: {result.success}")
    print(f"   ‚Ä¢ Status: {result.status}")
    print(f"   ‚Ä¢ Steps: {result.budget_used.steps_used}")
    print(f"   ‚Ä¢ Health: {health.status}")


def main():
    """Run all examples."""
    print("\n" + "üéØ " * 20)
    print("LangGraph + Multi-Provider LLM Integration Examples")
    print("üéØ " * 20)

    print("\nüí° Key Feature: LangGraph works with ANY LLM provider:")
    print("   ‚Ä¢ OpenAI (GPT-4)")
    print("   ‚Ä¢ Anthropic (Claude)")
    print("   ‚Ä¢ OpenRouter (100+ models)")
    print("   ‚Ä¢ Ollama (local, free)")
    print("   ‚Ä¢ Automatic fallback on failures")

    try:
        example_1_basic_any_provider()
    except Exception as e:
        print(f"\n‚ùå Example 1 failed: {e}")

    try:
        example_2_provider_fallback()
    except Exception as e:
        print(f"\n‚ùå Example 2 failed: {e}")

    try:
        example_3_full_integration()
    except Exception as e:
        print(f"\n‚ùå Example 3 failed: {e}")

    print("\n" + "=" * 60)
    print("‚úÖ Examples complete!")
    print("=" * 60)


if __name__ == "__main__":
    main()

