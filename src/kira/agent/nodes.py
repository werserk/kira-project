"""LangGraph nodes for agent execution flow.

Implements the core nodes: plan, reflect, tool, verify, and route.
"""

from __future__ import annotations

import json
import logging
import time
from typing import TYPE_CHECKING, Any

if TYPE_CHECKING:
    from ..adapters.llm import LLMAdapter, Message
    from .state import AgentState
    from .tools import ToolRegistry

logger = logging.getLogger(__name__)

__all__ = ["plan_node", "reflect_node", "tool_node", "verify_node", "respond_node", "route_node"]


def plan_node(state: AgentState, llm_adapter: LLMAdapter, tools_description: str) -> dict[str, Any]:
    """Planning node - generates execution plan from user request.

    Parameters
    ----------
    state
        Current agent state
    llm_adapter
        LLM adapter for generating plan
    tools_description
        Description of available tools

    Returns
    -------
    dict
        State updates with plan
    """
    logger.info(f"[{state.trace_id}] Planning phase started")
    state.status = "planning"

    # Build prompt for planning
    system_prompt = f"""You are Kira's AI planner. Generate a JSON execution plan for the user's request.

AVAILABLE TOOLS:
{tools_description}

OUTPUT FORMAT (JSON only):
{{
  "plan": ["step 1 description", "step 2 description", ...],
  "tool_calls": [
    {{"tool": "exact_tool_name", "args": {{}}, "dry_run": false}},
    ...
  ],
  "reasoning": "Brief explanation"
}}

RULES:
- Use EXACT tool names from the list above
- Set dry_run=false for actual execution (reflection_node will ensure safety)
- Only use dry_run=true if user explicitly asks to simulate/preview
- Keep plans concise (max {state.budget.max_steps - state.budget.steps_used} steps)
- Return ONLY valid JSON, no markdown or extra text
"""

    # Get last user message
    user_message = ""
    for msg in reversed(state.messages):
        if msg.get("role") == "user":
            user_message = msg.get("content", "")
            break

    if not user_message:
        logger.warning(f"[{state.trace_id}] No user message found")
        return {"error": "No user message to plan for", "status": "error"}

    # Call LLM
    try:
        from ..adapters.llm import Message

        messages = [
            Message(role="system", content=system_prompt),
            Message(role="user", content=user_message),
        ]

        response = llm_adapter.chat(messages, temperature=0.3, max_tokens=2000, timeout=30.0)

        # Update token budget
        tokens_used = response.usage.get("total_tokens", 0)
        state.budget.tokens_used += tokens_used

        # Parse plan
        plan_data = json.loads(response.content)
        logger.info(f"[{state.trace_id}] Generated plan with {len(plan_data.get('tool_calls', []))} steps")

        return {
            "plan": plan_data.get("tool_calls", []),
            "memory": {**state.memory, "reasoning": plan_data.get("reasoning", "")},
            "status": "planned",
        }

    except json.JSONDecodeError as e:
        logger.error(f"[{state.trace_id}] Failed to parse plan JSON: {e}")
        return {"error": f"Invalid plan JSON: {e}", "status": "error"}
    except Exception as e:
        logger.error(f"[{state.trace_id}] Planning failed: {e}", exc_info=True)
        return {"error": f"Planning failed: {e}", "status": "error"}


def reflect_node(state: AgentState, llm_adapter: LLMAdapter) -> dict[str, Any]:
    """Reflection node - reviews plan for safety and correctness.

    Parameters
    ----------
    state
        Current agent state
    llm_adapter
        LLM adapter for reflection

    Returns
    -------
    dict
        State updates with reflection results
    """
    logger.info(f"[{state.trace_id}] Reflection phase started")

    if not state.plan:
        logger.warning(f"[{state.trace_id}] No plan to reflect on")
        return {"status": "reflected"}

    # Build reflection prompt
    system_prompt = """You are a safety reviewer for AI agent plans. Review the plan and identify risks.

OUTPUT FORMAT (JSON only):
{
  "safe": true/false,
  "concerns": ["concern 1", "concern 2", ...],
  "revised_plan": [...],  // Only if unsafe and can be fixed
  "reasoning": "Brief explanation"
}

SAFETY CHECKS:
- No operations that could cause unintended data loss
- FSM state transitions are valid
- Arguments are reasonable
- User intent is clear (e.g., "delete all" requires explicit confirmation)
"""

    try:
        from ..adapters.llm import Message

        messages = [
            Message(role="system", content=system_prompt),
            Message(role="user", content=f"Review this plan:\n{json.dumps(state.plan, indent=2)}"),
        ]

        response = llm_adapter.chat(messages, temperature=0.1, max_tokens=1000, timeout=20.0)

        # Update token budget
        tokens_used = response.usage.get("total_tokens", 0)
        state.budget.tokens_used += tokens_used

        reflection = json.loads(response.content)
        is_safe = reflection.get("safe", True)

        logger.info(f"[{state.trace_id}] Reflection complete: safe={is_safe}")

        if not is_safe and reflection.get("revised_plan"):
            logger.warning(f"[{state.trace_id}] Plan revised due to safety concerns")
            return {
                "plan": reflection["revised_plan"],
                "memory": {**state.memory, "reflection": reflection},
                "status": "reflected",
            }

        return {
            "memory": {**state.memory, "reflection": reflection},
            "status": "reflected",
        }

    except Exception as e:
        logger.error(f"[{state.trace_id}] Reflection failed: {e}", exc_info=True)
        # Continue without reflection on error
        return {"status": "reflected"}


def tool_node(state: AgentState, tool_registry: ToolRegistry) -> dict[str, Any]:
    """Tool execution node - executes the current step.

    Parameters
    ----------
    state
        Current agent state
    tool_registry
        Registry of available tools

    Returns
    -------
    dict
        State updates with tool results
    """
    logger.info(f"[{state.trace_id}] Tool execution phase started (step {state.current_step})")
    state.status = "executing"

    if state.current_step >= len(state.plan):
        logger.warning(f"[{state.trace_id}] No more steps to execute")
        return {"status": "completed"}

    step = state.plan[state.current_step]
    tool_name = step.get("tool", "")
    args = step.get("args", {})
    dry_run = step.get("dry_run", False) or state.flags.dry_run

    logger.info(f"[{state.trace_id}] Executing tool: {tool_name} (dry_run={dry_run})")

    start_time = time.time()

    try:
        tool = tool_registry.get(tool_name)
        if not tool:
            available = [t.name for t in tool_registry.list_tools()]
            error_msg = f"Tool not found: {tool_name}. Available: {', '.join(available)}"
            logger.error(f"[{state.trace_id}] {error_msg}")
            return {
                "error": error_msg,
                "status": "error",
            }

        # Execute tool
        result = tool.execute(args, dry_run=dry_run)

        elapsed = time.time() - start_time
        state.budget.wall_time_used += elapsed
        state.budget.steps_used += 1

        # Add result to tool_results
        tool_result = {
            **result.to_dict(),
            "tool": tool_name,
            "step": state.current_step,
            "elapsed_ms": int(elapsed * 1000),
        }

        logger.info(
            f"[{state.trace_id}] Tool {tool_name} completed: status={result.status}, elapsed={elapsed:.2f}s"
        )

        return {
            "tool_results": state.tool_results + [tool_result],
            "current_step": state.current_step + 1,
            "status": "executed" if result.status == "ok" else "error",
            "error": result.error if result.status == "error" else None,
        }

    except Exception as e:
        logger.error(f"[{state.trace_id}] Tool execution failed: {e}", exc_info=True)
        elapsed = time.time() - start_time
        state.budget.wall_time_used += elapsed

        return {
            "error": f"Tool execution failed: {e}",
            "status": "error",
        }


def verify_node(state: AgentState, tool_registry: ToolRegistry) -> dict[str, Any]:
    """Verification node - validates execution results.

    Parameters
    ----------
    state
        Current agent state
    tool_registry
        Registry of available tools

    Returns
    -------
    dict
        State updates with verification results
    """
    logger.info(f"[{state.trace_id}] Verification phase started")
    state.status = "verifying"

    if not state.tool_results:
        logger.warning(f"[{state.trace_id}] No results to verify")
        return {"status": "verified"}

    # Simple verification: check if last operation succeeded
    last_result = state.tool_results[-1]
    if last_result.get("status") == "error":
        logger.warning(f"[{state.trace_id}] Last operation failed, verification skipped")
        return {"status": "verified"}

    # TODO: Add more sophisticated verification logic
    # - Check FSM state transitions
    # - Verify no duplicate operations
    # - Validate data integrity

    logger.info(f"[{state.trace_id}] Verification passed")
    return {"status": "verified"}


def respond_node(state: AgentState, llm_adapter: LLMAdapter) -> dict[str, Any]:
    """Response generation node - creates natural language response.

    Generates a conversational, natural language response based on:
    - Original user request
    - Execution plan
    - Tool results
    - Any errors

    Parameters
    ----------
    state
        Current agent state
    llm_adapter
        LLM adapter for generating response

    Returns
    -------
    dict
        State updates with natural language response
    """
    logger.info(f"[{state.trace_id}] Response generation phase started")
    state.status = "responding"

    # Extract user request (get the LAST user message, not first!)
    user_request = ""
    if state.messages:
        # Find the last user message in the conversation
        for msg in reversed(state.messages):
            if msg.get("role") == "user":
                user_request = msg.get("content", "")
                break
        # Fallback to first message if no user role found
        if not user_request:
            user_request = state.messages[0].get("content", "")

    # Build context for response generation
    context_parts = []

    # Add execution summary
    if state.tool_results:
        context_parts.append("EXECUTION RESULTS:")
        for i, result in enumerate(state.tool_results, 1):
            tool_name = result.get("tool", "unknown")
            status = result.get("status", "unknown")
            data = result.get("data", {})
            context_parts.append(f"{i}. Tool: {tool_name}")
            context_parts.append(f"   Status: {status}")
            if data:
                context_parts.append(f"   Data: {json.dumps(data, ensure_ascii=False)}")

    # Add error if present
    if state.error:
        context_parts.append(f"\nERROR: {state.error}")

    context = "\n".join(context_parts)

    # Build prompt for natural response
    system_prompt = """Ğ¢Ñ‹ - ĞšĞ¸Ñ€Ğ°, Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğ¹ Ğ¸ Ğ·Ğ°Ğ±Ğ¾Ñ‚Ğ»Ğ¸Ğ²Ñ‹Ğ¹ AI-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚. ĞĞ±Ñ‰Ğ°Ğ¹ÑÑ Ğ½Ğ° "Ñ‚Ñ‹", Ñ‚ĞµĞ¿Ğ»Ğ¾ Ğ¸ Ğ¿Ğ¾-Ñ‡ĞµĞ»Ğ¾Ğ²ĞµÑ‡ĞµÑĞºĞ¸.

Ğ’ĞĞ–ĞĞ:
- ĞĞ±Ñ€Ğ°Ñ‰Ğ°Ğ¹ÑÑ Ğ½Ğ° "Ñ‚Ñ‹" (Ğ½Ğµ "Ğ²Ñ‹")
- Ğ‘ÑƒĞ´ÑŒ Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ğ¾Ğ¹, Ñ‚ĞµĞ¿Ğ»Ğ¾Ğ¹, Ğ»Ğ°ÑĞºĞ¾Ğ²Ğ¾Ğ¹ Ğ¸ ÑƒÑĞ»ÑƒĞ¶Ğ»Ğ¸Ğ²Ğ¾Ğ¹
- ĞĞ• Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ ÑĞ¼Ğ¾Ğ´Ğ·Ğ¸ Ğ¸Ğ»Ğ¸ ÑĞ¼Ğ°Ğ¹Ğ»Ğ¸ĞºĞ¸ - Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‚ĞµĞºÑÑ‚
- ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ ĞºÑ€Ğ°Ñ‚ĞºĞ¾, Ğ½Ğ¾ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾
- Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ° Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ° - Ñ€Ğ°Ğ´ÑƒĞ¹ÑÑ Ğ²Ğ¼ĞµÑÑ‚Ğµ Ñ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¼
- Ğ•ÑĞ»Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° - Ğ¾Ğ±ÑŠÑÑĞ½Ğ¸ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½Ğ¾ Ğ¸ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğµ
- Ğ“Ğ¾Ğ²Ğ¾Ñ€Ğ¸ Ğ½Ğ° ÑĞ·Ñ‹ĞºĞµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ (Ñ€ÑƒÑÑĞºĞ¸Ğ¹/Ğ°Ğ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹)
- ĞĞµ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ¹ Ñ‚ĞµÑ…Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸ (Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ğ¾Ğ², ID)
- Ğ‘ÑƒĞ´ÑŒ ĞºĞ°Ğº Ğ½Ğ°ÑÑ‚Ğ¾ÑÑ‰Ğ¸Ğ¹ Ğ·Ğ°Ğ±Ğ¾Ñ‚Ğ»Ğ¸Ğ²Ñ‹Ğ¹ Ğ´Ñ€ÑƒĞ³-Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº

Ğ¡Ğ¢Ğ˜Ğ›Ğ¬:
âœ… "ĞÑ‚Ğ»Ğ¸Ñ‡Ğ½Ğ¾, Ñ Ğ½Ğ°ÑˆĞ»Ğ° 3 Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸! Ğ¥Ğ¾Ñ‡ĞµÑˆÑŒ, Ñ€Ğ°ÑÑĞºĞ°Ğ¶Ñƒ Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ?"
âœ… "Ğ¡ Ñ€Ğ°Ğ´Ğ¾ÑÑ‚ÑŒÑ Ğ¿Ğ¾Ğ¼Ğ¾Ğ³Ñƒ! Ğ£ Ñ‚ĞµĞ±Ñ ĞµÑÑ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ´Ğ°Ñ‡."
âŒ "ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ 3 Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸. Ğ’Ñ‹ Ğ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ´ĞµÑ‚Ğ°Ğ»Ğ¸."
âŒ "Ğ’Ğ¾Ñ‚ Ñ‚Ğ²Ğ¾Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ğŸ˜Š" (Ğ±ĞµĞ· ÑĞ¼Ğ¾Ğ´Ğ·Ğ¸!)

Ğ¢Ğ²Ğ¾Ñ Ñ†ĞµĞ»ÑŒ - Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ğ» Ñ‚ĞµĞ¿Ğ»Ğ¾Ñ‚Ñƒ Ğ¸ Ğ·Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ‡ĞµÑ€ĞµĞ· ÑĞ»Ğ¾Ğ²Ğ°, Ğ° Ğ½Ğµ ÑĞ¼Ğ°Ğ¹Ğ»Ğ¸ĞºĞ¸."""

    user_prompt = f"""ĞŸĞ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ ÑĞ¿Ñ€Ğ¾ÑĞ¸Ğ»: "{user_request}"

{context}

Ğ¡Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞ¹ Ğ´Ñ€ÑƒĞ¶ĞµĞ»ÑĞ±Ğ½Ñ‹Ğ¹ Ğ¸ ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ²Ñ‹ÑˆĞµ. ĞŸĞ¾Ğ¼Ğ½Ğ¸ - Ğ¾Ğ±Ñ‰Ğ°Ğ¹ÑÑ Ğ½Ğ° "Ñ‚Ñ‹" Ğ¸ Ğ±ÑƒĞ´ÑŒ Ñ‚ĞµĞ¿Ğ»Ğ¾Ğ¹!"""

    try:
        from ..adapters.llm import Message

        messages: list[Message] = [
            Message(role="system", content=system_prompt),
            Message(role="user", content=user_prompt),
        ]

        response = llm_adapter.chat(
            messages,
            temperature=0.8,  # Slightly higher for more natural responses
            max_tokens=500,
            timeout=15.0,
        )

        nl_response = response.content.strip()
        logger.info(f"[{state.trace_id}] Generated natural response: {nl_response[:100]}...")

        return {
            "response": nl_response,
            "status": "responded",
        }

    except Exception as e:
        logger.error(f"[{state.trace_id}] Response generation failed: {e}", exc_info=True)

        # Fallback to simple response
        fallback = "Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¾, Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¾!" if state.error is None else f"ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ¾ÑˆĞ»Ğ° Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {state.error}"

        return {
            "response": fallback,
            "status": "responded",
        }


def route_node(state: AgentState) -> str:
    """Routing node - decides next step based on state.

    Parameters
    ----------
    state
        Current agent state

    Returns
    -------
    str
        Name of next node to execute
    """
    logger.debug(f"[{state.trace_id}] Routing: status={state.status}, step={state.current_step}/{len(state.plan)}")

    # Check budget
    if state.budget.is_exceeded():
        logger.warning(f"[{state.trace_id}] Budget exceeded")
        return "halt"

    # Check for errors
    if state.error or state.status == "error":
        if state.retry_count < 2:
            logger.info(f"[{state.trace_id}] Error detected, retrying (attempt {state.retry_count + 1})")
            return "plan"  # Try to replan
        else:
            logger.error(f"[{state.trace_id}] Max retries reached")
            return "halt"

    # Route based on status
    if state.status == "pending":
        return "plan"
    elif state.status == "planned":
        if state.flags.enable_reflection:
            return "reflect"
        else:
            return "tool"
    elif state.status == "reflected":
        return "tool"
    elif state.status == "executed":
        if state.flags.enable_verification:
            return "verify"
        else:
            # Check if more steps remain
            if state.current_step < len(state.plan):
                return "tool"
            else:
                return "done"
    elif state.status == "verified":
        # Check if more steps remain
        if state.current_step < len(state.plan):
            return "tool"
        else:
            return "respond"  # Generate NL response before done
    elif state.status == "responded":
        return "done"
    elif state.status == "completed":
        return "respond"  # Always generate NL response
    else:
        logger.warning(f"[{state.trace_id}] Unknown status: {state.status}")
        return "halt"

